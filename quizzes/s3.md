# S3 Quiz

1. In S3 the durability of my files is ________.
    - 99.999999999 (11x9s)
2. You work for a major news network in Europe. They have just released a new mobile app that allows users to post their photos of newsworthy events in real time. Your organization expects this app to grow very quickly, essentially doubling its user base each month. The app uses S3 to store the images, and you are expecting sudden and sizable increases in traffic to S3 when a major news event takes place (as users will be uploading large amounts of content.) You need to keep your storage costs to a minimum, and it does not matter if some objects are lost. With these factors in mind, which storage media should you use to keep costs as low as possible?
    - S3 - One Zone Infrequent Access (this is the cheapest option available, since RRS is deprecated)
3. S3 has eventual consistency for which HTTP Methods?
    - overwrite PUTS and DELETES
4. S3 stands for...
    - Simple Storage Service
5. You run a meme creation website that stores the original images in S3 and each meme's meta data in DynamoDB. You need to decide upon a low-cost storage option for the memes, themselves. If a meme object is unavailable or lost, a Lambda function will automatically recreate it using the original file from S3 and the metadata from DynamoDB. Which storage solution should you use to store the non-critical, easily reproducible memes in the most cost effective way?
    - S3 - One Zone Infrequent Access (this is the cheapest option available, since RRS is deprecated)
6. You work for manufacturing company who operate a hybrid infrastructure with systems located both in a local Datacentre and in AWS, connected via Direct Connect. Currently, all on premise servers are backed up to a local NAS, but your CTO wants you decide on the best way to store copies of these backups in AWS. He has asked you to propose a solution which; maintains maximum durability, on-demand access to the files, but minimizes cost. Choose the best option from the following which meets the brief.
    - S3 - One Zone Infrequent Access (this is the cheapest option available and has the same availability as regular S3, but the access time is longer like glacier)
7. Your digital marketing company currently stores their data on premise, but they are considering migrating their data storage to S3. Each bucket will be named after an individual customer, followed by a random series of letters and numbers. Once written to S3, the customer data is rarely changed. However, on occasion, customers may need immediate access to certain files, which are not easily reproducible if lost. Which S3 storage class should you choose to minimise costs and to expedite a quick retrieval of your customers' data?
    - S3 - Infrequent Access (having all zones allows for faster access and as long as the access in infrequent, it's cheaper than S3 standard)
8. S3 has what consistency model for PUTS of new objects
    - Read after Write consistency
9. What is AWS Storage Gateway?
    - It is a virtual appliance that can be used to cache S3 locally at a customers site.
10. How many S3 buckets can I have per account by default?
    - 100 buckets
11. You have uploaded a file to S3. Which HTTP code would indicate that the upload was successful?
    - 200
12. You work for a health insurance company that amasses a large number of patients' health records. Each record will be used once when assessing a customer, and will then need to be securely stored for a period of 7 years. In some rare cases, you may need to retrieve this data within 24 hours of a claim being lodged. Given these requirements, which type of AWS storage would deliver the least expensive solution?
    - Glacier (24 hour recovery rate indicates this is the best answer; glacier takes about 5 hours for retrieval)
13. What is the availability of objects stored in S3?
    - 99.99% (don't get availability and durability mixed up; durability is 11x9s - availability is 4x9s)
14. What is the minimum file size that I can store on S3?
    - 0 bytes
15. One of your users is trying to upload a 7.5GB file to S3. However, they keep getting the following error message: "Your proposed upload exceeds the maximum allowed object size.". What solution to this problem does AWS recommend?
    - Design your application to use the Multipart Upload API for all objects.
16. You are a solutions architect who works with a large digital media company. The company has decided that they want to operate within the Japanese region and they need a bucket called "testbucket" set up immediately to test their web application on. You log in to the AWS console and try to create this bucket in the Japanese region however you are told that the bucket name is already taken. What should you do to resolve this?
    - Bucket names are global, so choose another bucket name.
17. What is the bucket name + region convention?
    - https://s3-region.amazonaws.com/bucket-name
18. What is Amazon Glacier?
    - Long term data archival service
19. You run a popular photo sharing website that depends on S3 to store content. Paid advertising is your primary source of revenue. However, you have discovered that other websites are linking directly to the images in your buckets, not to the HTML pages that serve the content. This means that people are not seeing the paid advertising, and you are paying AWS unnecessarily to serve content directly from S3. How might you resolve this issue?
    - Remove the ability for images to be served publicly to the site and then use signed URLs with expiry dates.
20. What is the availability of S3-OneZone-IA?
    - 99.50% (lower availability but same durability, hence less availability than 99.99% like S3 standard)
21. You have been asked to advise on a scaling concern. The client has a elegant solution that works well. As the information base grows they use CloudFormation to spin up another stack made up of an S3 bucket and supporting compute instances. The trigger for creating a new stack is when the PUT rate approaches 100 PUTs per second. the problem is that as the business grows that number of buckets is growing into the hundreds and will soon be in the thousands. You have been asked what can be done to reduce the number of buckets without changing the basic architecture.
    - Change the trigger level to around 3000 as S3 can now accommodate much higher PUT and GET levels. (Until 2018 there was a hard limit on S3 puts of 100 PUTs per second. To achieve this care needed to be taken with the structure of the name Key to ensure parallel processing. As of July 2018 the limit was raised to 3500 and the need for the Key design was basically eliminated. Disk IOPS is not the issue with the problem. The account limit is not the issue with the problem.)